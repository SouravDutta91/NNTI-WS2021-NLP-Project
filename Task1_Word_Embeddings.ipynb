{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VZXi_KGi0UR"
   },
   "source": [
    "# Task 1: Word Embeddings (10 points)\n",
    "\n",
    "This notebook will guide you through all steps necessary to train a word2vec model (Detailed description in the PDF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48t-II1vkuau"
   },
   "source": [
    "## Imports\n",
    "\n",
    "This code block is reserved for your imports. \n",
    "\n",
    "You are free to use the following packages: \n",
    "\n",
    "(List of packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4kh6nh84-AOL"
   },
   "outputs": [],
   "source": [
    "import re,string\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pickle\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='2,3,4,5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "GPU found\n",
      "Using GPU at cuda: 0\n",
      "=================================\n",
      " \n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device == 'cuda':\n",
    "  print(\"=================================\")\n",
    "  print(\"GPU found\")\n",
    "  print(\"Using GPU at cuda:\",torch.cuda.current_device())\n",
    "  print(\"=================================\")\n",
    "  print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWmk3hVllEcU"
   },
   "source": [
    "# 1.1 Get the data (0.5 points)\n",
    "\n",
    "The Hindi portion HASOC corpus from [github.io](https://hasocfire.github.io/hasoc/2019/dataset.html) is already available in the repo, at data/hindi_hatespeech.tsv . Load it into a data structure of your choice. Then, split off a small part of the corpus as a development set (~100 data points).\n",
    "\n",
    "If you are using Colab the first two lines will let you upload folders or files from your local file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "XtI7DJ-0-AOP"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://raw.githubusercontent.com/SouravDutta91/NNTI-WS2021-NLP-Project/main/data/hindi_hatespeech.tsv',sep='\\t')\n",
    "data_text = data['text']\n",
    "textd = data_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "      <th>task_1</th>\n",
       "      <th>task_2</th>\n",
       "      <th>task_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hasoc_hi_5556</td>\n",
       "      <td>बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hasoc_hi_5648</td>\n",
       "      <td>सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>UNT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hasoc_hi_164</td>\n",
       "      <td>तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>TIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hasoc_hi_3530</td>\n",
       "      <td>बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hasoc_hi_5206</td>\n",
       "      <td>चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hasoc_hi_5121</td>\n",
       "      <td>मुंबई में बारिश से लोगों को काफी समस्या हो रही है</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hasoc_hi_7142</td>\n",
       "      <td>Ahmed's dad:-- beta aaj teri mammy kyu nahi ba...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hasoc_hi_4321</td>\n",
       "      <td>5 लाख मुसलमान उर्स में, अजमेर की दरगाह पर आते ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hasoc_hi_4674</td>\n",
       "      <td>Do mahashaktiyan mili hain, charo taraf khusi ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hasoc_hi_1637</td>\n",
       "      <td>Chants of 'Jai Sri Ram' as Owaisi takes oath: ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text_id                                               text task_1  \\\n",
       "0  hasoc_hi_5556  बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...    NOT   \n",
       "1  hasoc_hi_5648  सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...    HOF   \n",
       "2   hasoc_hi_164  तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...    HOF   \n",
       "3  hasoc_hi_3530  बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...    NOT   \n",
       "4  hasoc_hi_5206  चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...    NOT   \n",
       "5  hasoc_hi_5121  मुंबई में बारिश से लोगों को काफी समस्या हो रही है    NOT   \n",
       "6  hasoc_hi_7142  Ahmed's dad:-- beta aaj teri mammy kyu nahi ba...    NOT   \n",
       "7  hasoc_hi_4321  5 लाख मुसलमान उर्स में, अजमेर की दरगाह पर आते ...    NOT   \n",
       "8  hasoc_hi_4674  Do mahashaktiyan mili hain, charo taraf khusi ...    NOT   \n",
       "9  hasoc_hi_1637  Chants of 'Jai Sri Ram' as Owaisi takes oath: ...    NOT   \n",
       "\n",
       "  task_2 task_3  \n",
       "0   NONE   NONE  \n",
       "1   PRFN    UNT  \n",
       "2   PRFN    TIN  \n",
       "3   NONE   NONE  \n",
       "4   NONE   NONE  \n",
       "5   NONE   NONE  \n",
       "6   NONE   NONE  \n",
       "7   NONE   NONE  \n",
       "8   NONE   NONE  \n",
       "9   NONE   NONE  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-mSJ8nUlupB"
   },
   "source": [
    "## 1.2 Data preparation (0.5 + 0.5 points)\n",
    "\n",
    "* Prepare the data by removing everything that does not contain information. \n",
    "User names (starting with '@') and punctuation symbols clearly do not convey information, but we also want to get rid of so-called [stopwords](https://en.wikipedia.org/wiki/Stop_word), i. e. words that have little to no semantic content (and, but, yes, the...). Hindi stopwords can be found [here](https://github.com/stopwords-iso/stopwords-hi/blob/master/stopwords-hi.txt) Then, standardize the spelling by lowercasing all words.\n",
    "Do this for the development section of the corpus for now.\n",
    "\n",
    "* What about hashtags (starting with '#') and emojis? Should they be removed too? Justify your answer in the report, and explain how you accounted for this in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctuations_remove(input):\n",
    "  output = \"\".join([x for x in input if x not in string.punctuation])\n",
    "  return output\n",
    "\n",
    "def numbers_remove(input):\n",
    "  output = re.sub(r\"[0-9]+\", \"\", input)\n",
    "  return output\n",
    "\n",
    "def usernames_remove(input):\n",
    "  output = re.sub(r\"@\\S+\", \"\", input)\n",
    "  return output\n",
    "\n",
    "def hashtag_remove(input):\n",
    "  output = re.sub(r\"#\\S+\", \"\", input)\n",
    "  return output\n",
    "\n",
    "def http_remove(input):\n",
    "  output = re.sub(r\"http\\S+\", \"\", input)\n",
    "  return output\n",
    "\n",
    "def emojis_remove(input):\n",
    "  EMOJI_PATTERN = re.compile(\n",
    "      \"[\"\n",
    "      \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "      \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "      \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "      \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "      \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "      \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "      \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "      \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "      \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "      \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "      \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "      \"\\U000024C2-\\U0001F251\" \n",
    "      \"]+\"\n",
    "  )\n",
    "  \n",
    "  output = EMOJI_PATTERN.sub(r'',input)\n",
    "  return output\n",
    "\n",
    "def extra_whitespaces(input):\n",
    "  output = input.replace('\\s+', ' ')\n",
    "  return output\n",
    "\n",
    "def stopwords_remove(m):\n",
    "  hindi_stopwords = pd.read_csv('https://raw.githubusercontent.com/stopwords-iso/stopwords-hi/master/stopwords-hi.txt').stack().tolist()\n",
    "  english_stopwords = pd.read_csv('https://raw.githubusercontent.com/stopwords-iso/stopwords-en/master/stopwords-en.txt').stack().tolist()\n",
    "  stopwords = hindi_stopwords + english_stopwords\n",
    "\n",
    "  output = pd.Series(m).apply(lambda x: [item for item in x.split() if item not in stopwords])\n",
    "  return output\n",
    "\n",
    "def tolower(input):\n",
    "  output = input.lower()\n",
    "  return output\n",
    "\n",
    "def corpus_preprocess(corpus):\n",
    "  corpus = corpus.apply(lambda x: tolower(x))\n",
    "  corpus = corpus.apply(lambda x: emojis_remove(x))\n",
    "  corpus = corpus.apply(lambda x: http_remove(x))\n",
    "  corpus = corpus.apply(lambda x: hashtag_remove(x))\n",
    "  corpus = corpus.apply(lambda x: numbers_remove(x))\n",
    "  corpus = corpus.apply(lambda x: usernames_remove(x))\n",
    "  corpus = corpus.apply(lambda x: punctuations_remove(x))\n",
    "  corpus = corpus.apply(lambda x: stopwords_remove(x))\n",
    "  corpus = corpus.apply(lambda x: extra_whitespaces(x))\n",
    "  return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Preprocessing\n",
      "Preprocessing ended!\n",
      "Pre-processing the text took 0:00:08.811462\n",
      "===========================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Started Preprocessing\")\n",
    "cleanstart = time.time()\n",
    "cleantext = corpus_preprocess(textd)\n",
    "cleanend = str(datetime.timedelta(seconds = time.time()-cleanstart))\n",
    "print(\"Preprocessing ended!\")\n",
    "print(\"Pre-processing the text took {}\".format(cleanend))\n",
    "print(\"===========================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(text):\n",
    "  c = []\n",
    "  for sent in text[0]:\n",
    "    a = \" \".join(sent)\n",
    "    c.append(a)\n",
    "  df_text = pd.DataFrame(c,columns=[\"text\"])\n",
    "  return df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =  tokens(cleantext)\n",
    "text = text['text'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           [बांग्लादेश, शानदार, वापसी, भारत, रन, रोका]\n",
       "1     [सब, रंडी, नाच, देखने, व्यस्त, होगा, सब, शुरू,...\n",
       "2     [तुम, हरामियों, बस, जूतों, कमी, शुक्र, तुम्हार...\n",
       "3     [बीजेपी, mla, आकाश, विजयवर्गीय, जेल, रिहा, जमा...\n",
       "4     [चमकी, बुखार, विधानसभा, परिसर, आरजेडी, प्रदर्श...\n",
       "                            ...                        \n",
       "95    [देश, पहली, बार, सरकार, प्रो, इंकम्बेंसी, जनाद...\n",
       "96    [आदमी, आदमी, मैं, पानी, बारे, सोचता, थालिखने, ...\n",
       "97    [मादरजात, सनी, तेरे, पास, टाइम, नही, तोतेरी, म...\n",
       "98    [थोर, क्रांतिकारक, राणी, लक्ष्मीबाई, यांना, पु...\n",
       "99    [मुस्लिम, लोगों, वोट, मांगने, वाली, पार्टियां,...\n",
       "Name: text, Length: 100, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Je09nozLmmMm"
   },
   "source": [
    "## 1.3 Build the vocabulary (0.5 + 0.5 points)\n",
    "\n",
    "The input to the first layer of word2vec is an one-hot encoding of the current word. The output od the model is then compared to a numeric class label of the words within the size of the skip-gram window. Now\n",
    "\n",
    "* Compile a list of all words in the development section of your corpus and save it in a variable ```V```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "VpoGmTKx-AOQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words are:  1097\n"
     ]
    }
   ],
   "source": [
    "V = list(set(text.sum())) #List of unique words in the corpus\n",
    "all_words = list(text.sum()) #All the words without removing duplicates\n",
    "print(\"Total number of unique words are: \",len(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['स्पेशली',\n",
       " 'दी।',\n",
       " 'hai',\n",
       " 'पहचान',\n",
       " 'बताया',\n",
       " 'khus',\n",
       " 'साहिब',\n",
       " 'जाती',\n",
       " 'नया',\n",
       " 'श्रद्धांजलि',\n",
       " 'जनता',\n",
       " 'कप',\n",
       " 'सवाल',\n",
       " 'तैसी',\n",
       " 'wale',\n",
       " 'हैँ',\n",
       " 'भारतीय',\n",
       " 'समर्थक',\n",
       " 'शत्शत्',\n",
       " 'तुमहृदय']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionaries of words and their indexes\n",
    "word_index = {word: i for i,word in enumerate(V)}\n",
    "index_word = {i: word for i,word in enumerate(V)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index['मुखिया']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'जाती'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_word[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiaVglVNoENY"
   },
   "source": [
    "* Then, write a function ```word_to_one_hot``` that returns a one-hot encoding of an arbitrary word in the vocabulary. The size of the one-hot encoding should be ```len(v)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "yqPNw6IT-AOQ"
   },
   "outputs": [],
   "source": [
    "def word_to_one_hot(word):\n",
    "  id = V.index(word)\n",
    "  onehot = [0.] * len(V)\n",
    "  onehot[id] = 1.\n",
    "  return torch.tensor(onehot)\n",
    "\n",
    "get_onehot = dict((word, word_to_one_hot(word)) for word in V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_onehot['मुखिया']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKD8zBlxVclh"
   },
   "source": [
    "## 1.4 Subsampling (0.5 points)\n",
    "\n",
    "The probability to keep a word in a context is given by:\n",
    "\n",
    "$P_{keep}(w_i) = \\Big(\\sqrt{\\frac{z(w_i)}{0.001}}+1\\Big) \\cdot \\frac{0.001}{z(w_i)}$\n",
    "\n",
    "Where $z(w_i)$ is the relative frequency of the word $w_i$ in the corpus. Now,\n",
    "* Calculate word frequencies\n",
    "* Define a function ```sampling_prob``` that takes a word (string) as input and returns the probabiliy to **keep** the word in a context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Mj4sDOVMMr0b"
   },
   "outputs": [],
   "source": [
    "def sampling_prob(word):\n",
    "  if word in all_words:\n",
    "    count = all_words.count(word)\n",
    "    zw_i = count / len(all_words)\n",
    "    p_wi_keep = (np.sqrt(zw_i/0.001) + 1)*(0.001/zw_i)\n",
    "  else:\n",
    "    p_wi_keep = 0\n",
    "  return p_wi_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7641229712289954"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_prob('मुखिया')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxV1P90zplxu"
   },
   "source": [
    "# 1.5 Skip-Grams (1 point)\n",
    "\n",
    "Now that you have the vocabulary and one-hot encodings at hand, you can start to do the actual work. The skip gram model requires training data of the shape ```(current_word, context)```, with ```context``` being the words before and/or after ```current_word``` within ```window_size```. \n",
    "\n",
    "* Have closer look on the original paper. If you feel to understand how skip-gram works, implement a function ```get_target_context``` that takes a sentence as input and [yield](https://docs.python.org/3.9/reference/simple_stmts.html#the-yield-statement)s a ```(current_word, context)```.\n",
    "\n",
    "* Use your ```sampling_prob``` function to drop words from contexts as you sample them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "r8CCTpVy-AOR"
   },
   "outputs": [],
   "source": [
    "def get_target_context(sentence,window):\n",
    "  thres = np.random.random()\n",
    "\n",
    "  for i,word in enumerate(sentence):\n",
    "    target = word_index[sentence[i]]\n",
    "\n",
    "    for j in range(i - window, i + window):\n",
    "      if j!=i and j <= len(sentence)-1 and j>=0:\n",
    "        if sampling_prob(sentence[j]) > thres:\n",
    "          context = word_index[sentence[j]]\n",
    "          yield target,context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfEFgtkmuDjL"
   },
   "source": [
    "# 1.6 Hyperparameters (0.5 points)\n",
    "\n",
    "According to the word2vec paper, what would be a good choice for the following hyperparameters? \n",
    "\n",
    "* Embedding dimension\n",
    "* Window size\n",
    "\n",
    "Initialize them in a dictionary or as independent variables in the code block below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "d7xSKuFJcYoD"
   },
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "window_size = 2\n",
    "embedding_size = 300\n",
    "\n",
    "# More hyperparameters\n",
    "learning_rate = 0.05\n",
    "epochs = 300\n",
    "batch_size = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiM2zq-YunPx"
   },
   "source": [
    "# 1.7 Pytorch Module (0.5 + 0.5 + 0.5 points)\n",
    "\n",
    "Pytorch provides a wrapper for your fancy and super-complex models: [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). The code block below contains a skeleton for such a wrapper. Now,\n",
    "\n",
    "* Initialize the two weight matrices of word2vec as fields of the class.\n",
    "\n",
    "* Override the ```forward``` method of this class. It should take a one-hot encoding as input, perform the matrix multiplications, and finally apply a log softmax on the output layer.\n",
    "\n",
    "* Initialize the model and save its weights in a variable. The Pytorch documentation will tell you how to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9sGNytYhwxS",
    "outputId": "41645b64-e4ed-4e6a-e10f-74cb39b92230"
   },
   "outputs": [],
   "source": [
    "# Create model \n",
    "class Word2Vec(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.v_len = len(V)\n",
    "    self.es = embedding_size\n",
    "    self.epochs = epochs\n",
    "    \n",
    "    self.w1 = nn.Linear(len(V),embedding_size,False)\n",
    "    self.w2 = nn.Linear(embedding_size,len(V))\n",
    "    self.soft = nn.LogSoftmax(dim = 1)\n",
    "\n",
    "  def forward(self, one_hot):\n",
    "    one_hot = self.w1(one_hot)\n",
    "    one_hot=self.w2(one_hot)\n",
    "    output=self.soft(one_hot)\n",
    "    return output.cuda()\n",
    "\n",
    "  def softmax(self,input):    \n",
    "    output = self.soft(input)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XefIDMMHv5zJ"
   },
   "source": [
    "# 1.8 Loss function and optimizer (0.5 points)\n",
    "\n",
    "Initialize variables with [optimizer](https://pytorch.org/docs/stable/optim.html#module-torch.optim) and loss function. You can take what is used in the word2vec paper, but you can use alternative optimizers/loss functions if you explain your choice in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "V9-Ino-e29w3"
   },
   "outputs": [],
   "source": [
    "# Define optimizer and loss\n",
    "model = Word2Vec().cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "The Word2Vec model: \n",
      "Word2Vec(\n",
      "  (w1): Linear(in_features=1097, out_features=300, bias=False)\n",
      "  (w2): Linear(in_features=300, out_features=1097, bias=True)\n",
      "  (soft): LogSoftmax(dim=1)\n",
      ")\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=====================================\")\n",
    "print(\"The Word2Vec model: \")\n",
    "print(model)\n",
    "print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckTfK78Ew8wI"
   },
   "source": [
    "# 1.9 Training the model (3 points)\n",
    "\n",
    "As everything is prepared, implement a training loop that performs several passes of the data set through the model. You are free to do this as you please, but your code should:\n",
    "\n",
    "* Load the weights saved in 1.6 at the start of every execution of the code block\n",
    "* Print the accumulated loss at least after every epoch (the accumulate loss should be reset after every epoch)\n",
    "* Define a criterion for the training procedure to terminate if a certain loss value is reached. You can find the threshold by observing the loss for the development set.\n",
    "\n",
    "You can play around with the number of epochs and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Gets the corpus and creates the training data with the target and its context\n",
    "and returns a dataframe containing them in terms of their indexes.\n",
    "'''\n",
    "def get_training_data(corpus,window):\n",
    "  t,c = [],[]\n",
    "  for sentence in corpus:\n",
    "    data = get_target_context(sentence,window)\n",
    "    for i,j in data:\n",
    "      x = get_onehot[index_word[i]]\n",
    "      t.append(x)\n",
    "      c.append(j)\n",
    "  t_data = pd.DataFrame(list(zip(t,c)),columns=[\"target\",\"context\"])\n",
    "  return t_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "LbMGD5L0mLDx"
   },
   "outputs": [],
   "source": [
    "def train(traindata,batchsize):\n",
    "  losses = []\n",
    "  print(\"Training started\")\n",
    "  for epoch in range(1,epochs+1):\n",
    "    total_loss = []\n",
    "    for wt,wc in zip(DataLoader(traindata.target.values,batch_size=batchsize),\n",
    "                     DataLoader(traindata.context.values,batch_size=batchsize)):\n",
    "      wt = wt.cuda()\n",
    "      wc = wc.cuda()\n",
    "      optimizer.zero_grad()\n",
    "      output = model(wt)\n",
    "      loss = criterion(output,wc)\n",
    "      total_loss.append(loss.item())\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    if epoch % 50 == 0 :\n",
    "      start = time.time()\n",
    "      print(\"===========================================================\")\n",
    "      print(\"Saving the model state\")\n",
    "      save_model(epoch)\n",
    "      end = str(datetime.timedelta(seconds = time.time()-start))\n",
    "      print(\"Model state saved. It was completed in {}\".format(end))      \n",
    "      print(\"===========================================================\")\n",
    "\n",
    "    if np.mean(total_loss) < 1.2:\n",
    "      break;\n",
    "    print(\"At epoch {} the loss is {}\".format(epoch ,round(np.mean(total_loss),3)))\n",
    "    losses.append(np.mean(total_loss))\n",
    "\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(\"LOSS\")\n",
    "  plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(epoch):\n",
    "  torch.save(model.state_dict(),\"epoch_{}.pt\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================\n",
      "Collecting training data\n",
      "It took 0:00:00.318723 to collect the data\n",
      "The training data has 4028 target-context pairs\n",
      "===================================================\n",
      "Training started\n",
      "At epoch 1 the loss is 7.006\n",
      "At epoch 2 the loss is 6.99\n",
      "At epoch 3 the loss is 6.974\n",
      "At epoch 4 the loss is 6.96\n",
      "At epoch 5 the loss is 6.946\n",
      "At epoch 6 the loss is 6.932\n",
      "At epoch 7 the loss is 6.92\n",
      "At epoch 8 the loss is 6.907\n",
      "At epoch 9 the loss is 6.895\n",
      "At epoch 10 the loss is 6.884\n",
      "At epoch 11 the loss is 6.872\n",
      "At epoch 12 the loss is 6.861\n",
      "At epoch 13 the loss is 6.849\n",
      "At epoch 14 the loss is 6.837\n",
      "At epoch 15 the loss is 6.825\n",
      "At epoch 16 the loss is 6.812\n",
      "At epoch 17 the loss is 6.798\n",
      "At epoch 18 the loss is 6.783\n",
      "At epoch 19 the loss is 6.767\n",
      "At epoch 20 the loss is 6.751\n",
      "At epoch 21 the loss is 6.736\n",
      "At epoch 22 the loss is 6.723\n",
      "At epoch 23 the loss is 6.711\n",
      "At epoch 24 the loss is 6.699\n",
      "At epoch 25 the loss is 6.688\n",
      "At epoch 26 the loss is 6.677\n",
      "At epoch 27 the loss is 6.666\n",
      "At epoch 28 the loss is 6.654\n",
      "At epoch 29 the loss is 6.643\n",
      "At epoch 30 the loss is 6.631\n",
      "At epoch 31 the loss is 6.618\n",
      "At epoch 32 the loss is 6.605\n",
      "At epoch 33 the loss is 6.591\n",
      "At epoch 34 the loss is 6.576\n",
      "At epoch 35 the loss is 6.561\n",
      "At epoch 36 the loss is 6.544\n",
      "At epoch 37 the loss is 6.527\n",
      "At epoch 38 the loss is 6.508\n",
      "At epoch 39 the loss is 6.489\n",
      "At epoch 40 the loss is 6.468\n",
      "At epoch 41 the loss is 6.446\n",
      "At epoch 42 the loss is 6.423\n",
      "At epoch 43 the loss is 6.398\n",
      "At epoch 44 the loss is 6.373\n",
      "At epoch 45 the loss is 6.345\n",
      "At epoch 46 the loss is 6.317\n",
      "At epoch 47 the loss is 6.287\n",
      "At epoch 48 the loss is 6.256\n",
      "At epoch 49 the loss is 6.224\n",
      "===========================================================\n",
      "Saving the model state\n",
      "Model state saved. It was completed in 0:00:00.011237\n",
      "===========================================================\n",
      "At epoch 50 the loss is 6.19\n",
      "At epoch 51 the loss is 6.155\n",
      "At epoch 52 the loss is 6.118\n",
      "At epoch 53 the loss is 6.08\n",
      "At epoch 54 the loss is 6.04\n",
      "At epoch 55 the loss is 5.999\n",
      "At epoch 56 the loss is 5.956\n",
      "At epoch 57 the loss is 5.912\n",
      "At epoch 58 the loss is 5.867\n",
      "At epoch 59 the loss is 5.82\n",
      "At epoch 60 the loss is 5.772\n",
      "At epoch 61 the loss is 5.722\n",
      "At epoch 62 the loss is 5.671\n",
      "At epoch 63 the loss is 5.619\n",
      "At epoch 64 the loss is 5.565\n",
      "At epoch 65 the loss is 5.51\n",
      "At epoch 66 the loss is 5.454\n",
      "At epoch 67 the loss is 5.397\n",
      "At epoch 68 the loss is 5.339\n",
      "At epoch 69 the loss is 5.28\n",
      "At epoch 70 the loss is 5.219\n",
      "At epoch 71 the loss is 5.159\n",
      "At epoch 72 the loss is 5.097\n",
      "At epoch 73 the loss is 5.035\n",
      "At epoch 74 the loss is 4.972\n",
      "At epoch 75 the loss is 4.909\n",
      "At epoch 76 the loss is 4.845\n",
      "At epoch 77 the loss is 4.781\n",
      "At epoch 78 the loss is 4.717\n",
      "At epoch 79 the loss is 4.652\n",
      "At epoch 80 the loss is 4.587\n",
      "At epoch 81 the loss is 4.522\n",
      "At epoch 82 the loss is 4.458\n",
      "At epoch 83 the loss is 4.393\n",
      "At epoch 84 the loss is 4.328\n",
      "At epoch 85 the loss is 4.263\n",
      "At epoch 86 the loss is 4.198\n",
      "At epoch 87 the loss is 4.134\n",
      "At epoch 88 the loss is 4.07\n",
      "At epoch 89 the loss is 4.005\n",
      "At epoch 90 the loss is 3.942\n",
      "At epoch 91 the loss is 3.878\n",
      "At epoch 92 the loss is 3.815\n",
      "At epoch 93 the loss is 3.752\n",
      "At epoch 94 the loss is 3.69\n",
      "At epoch 95 the loss is 3.628\n",
      "At epoch 96 the loss is 3.567\n",
      "At epoch 97 the loss is 3.506\n",
      "At epoch 98 the loss is 3.446\n",
      "At epoch 99 the loss is 3.386\n",
      "===========================================================\n",
      "Saving the model state\n",
      "Model state saved. It was completed in 0:00:00.006575\n",
      "===========================================================\n",
      "At epoch 100 the loss is 3.326\n",
      "At epoch 101 the loss is 3.268\n",
      "At epoch 102 the loss is 3.21\n",
      "At epoch 103 the loss is 3.153\n",
      "At epoch 104 the loss is 3.096\n",
      "At epoch 105 the loss is 3.04\n",
      "At epoch 106 the loss is 2.985\n",
      "At epoch 107 the loss is 2.931\n",
      "At epoch 108 the loss is 2.877\n",
      "At epoch 109 the loss is 2.825\n",
      "At epoch 110 the loss is 2.773\n",
      "At epoch 111 the loss is 2.722\n",
      "At epoch 112 the loss is 2.673\n",
      "At epoch 113 the loss is 2.624\n",
      "At epoch 114 the loss is 2.576\n",
      "At epoch 115 the loss is 2.53\n",
      "At epoch 116 the loss is 2.484\n",
      "At epoch 117 the loss is 2.44\n",
      "At epoch 118 the loss is 2.398\n",
      "At epoch 119 the loss is 2.356\n",
      "At epoch 120 the loss is 2.316\n",
      "At epoch 121 the loss is 2.278\n",
      "At epoch 122 the loss is 2.241\n",
      "At epoch 123 the loss is 2.206\n",
      "At epoch 124 the loss is 2.172\n",
      "At epoch 125 the loss is 2.14\n",
      "At epoch 126 the loss is 2.109\n",
      "At epoch 127 the loss is 2.081\n",
      "At epoch 128 the loss is 2.053\n",
      "At epoch 129 the loss is 2.028\n",
      "At epoch 130 the loss is 2.003\n",
      "At epoch 131 the loss is 1.98\n",
      "At epoch 132 the loss is 1.959\n",
      "At epoch 133 the loss is 1.938\n",
      "At epoch 134 the loss is 1.919\n",
      "At epoch 135 the loss is 1.9\n",
      "At epoch 136 the loss is 1.883\n",
      "At epoch 137 the loss is 1.867\n",
      "At epoch 138 the loss is 1.851\n",
      "At epoch 139 the loss is 1.836\n",
      "At epoch 140 the loss is 1.822\n",
      "At epoch 141 the loss is 1.809\n",
      "At epoch 142 the loss is 1.797\n",
      "At epoch 143 the loss is 1.785\n",
      "At epoch 144 the loss is 1.773\n",
      "At epoch 145 the loss is 1.763\n",
      "At epoch 146 the loss is 1.752\n",
      "At epoch 147 the loss is 1.742\n",
      "At epoch 148 the loss is 1.733\n",
      "At epoch 149 the loss is 1.724\n",
      "===========================================================\n",
      "Saving the model state\n",
      "Model state saved. It was completed in 0:00:00.010483\n",
      "===========================================================\n",
      "At epoch 150 the loss is 1.716\n",
      "At epoch 151 the loss is 1.708\n",
      "At epoch 152 the loss is 1.7\n",
      "At epoch 153 the loss is 1.693\n",
      "At epoch 154 the loss is 1.686\n",
      "At epoch 155 the loss is 1.679\n",
      "At epoch 156 the loss is 1.673\n",
      "At epoch 157 the loss is 1.667\n",
      "At epoch 158 the loss is 1.661\n",
      "At epoch 159 the loss is 1.656\n",
      "At epoch 160 the loss is 1.65\n",
      "At epoch 161 the loss is 1.645\n",
      "At epoch 162 the loss is 1.64\n",
      "At epoch 163 the loss is 1.636\n",
      "At epoch 164 the loss is 1.631\n",
      "At epoch 165 the loss is 1.627\n",
      "At epoch 166 the loss is 1.623\n",
      "At epoch 167 the loss is 1.619\n",
      "At epoch 168 the loss is 1.616\n",
      "At epoch 169 the loss is 1.612\n",
      "At epoch 170 the loss is 1.609\n",
      "At epoch 171 the loss is 1.605\n",
      "At epoch 172 the loss is 1.602\n",
      "At epoch 173 the loss is 1.599\n",
      "At epoch 174 the loss is 1.596\n",
      "At epoch 175 the loss is 1.593\n",
      "At epoch 176 the loss is 1.591\n",
      "At epoch 177 the loss is 1.588\n",
      "At epoch 178 the loss is 1.586\n",
      "At epoch 179 the loss is 1.583\n",
      "At epoch 180 the loss is 1.581\n",
      "At epoch 181 the loss is 1.579\n",
      "At epoch 182 the loss is 1.577\n",
      "At epoch 183 the loss is 1.574\n",
      "At epoch 184 the loss is 1.572\n",
      "At epoch 185 the loss is 1.571\n",
      "At epoch 186 the loss is 1.569\n",
      "At epoch 187 the loss is 1.567\n",
      "At epoch 188 the loss is 1.565\n",
      "At epoch 189 the loss is 1.564\n",
      "At epoch 190 the loss is 1.562\n",
      "At epoch 191 the loss is 1.56\n",
      "At epoch 192 the loss is 1.559\n",
      "At epoch 193 the loss is 1.558\n",
      "At epoch 194 the loss is 1.556\n",
      "At epoch 195 the loss is 1.555\n",
      "At epoch 196 the loss is 1.553\n",
      "At epoch 197 the loss is 1.552\n",
      "At epoch 198 the loss is 1.551\n",
      "At epoch 199 the loss is 1.55\n",
      "===========================================================\n",
      "Saving the model state\n",
      "Model state saved. It was completed in 0:00:00.007587\n",
      "===========================================================\n",
      "At epoch 200 the loss is 1.549\n",
      "At epoch 201 the loss is 1.548\n",
      "At epoch 202 the loss is 1.547\n",
      "At epoch 203 the loss is 1.546\n",
      "At epoch 204 the loss is 1.545\n",
      "At epoch 205 the loss is 1.544\n",
      "At epoch 206 the loss is 1.543\n",
      "At epoch 207 the loss is 1.542\n",
      "At epoch 208 the loss is 1.541\n",
      "At epoch 209 the loss is 1.54\n",
      "At epoch 210 the loss is 1.539\n",
      "At epoch 211 the loss is 1.538\n",
      "At epoch 212 the loss is 1.537\n",
      "At epoch 213 the loss is 1.537\n",
      "At epoch 214 the loss is 1.536\n",
      "At epoch 215 the loss is 1.535\n",
      "At epoch 216 the loss is 1.534\n",
      "At epoch 217 the loss is 1.534\n",
      "At epoch 218 the loss is 1.533\n",
      "At epoch 219 the loss is 1.532\n",
      "At epoch 220 the loss is 1.532\n",
      "At epoch 221 the loss is 1.531\n",
      "At epoch 222 the loss is 1.53\n",
      "At epoch 223 the loss is 1.53\n",
      "At epoch 224 the loss is 1.529\n",
      "At epoch 225 the loss is 1.529\n",
      "At epoch 226 the loss is 1.528\n",
      "At epoch 227 the loss is 1.528\n",
      "At epoch 228 the loss is 1.527\n",
      "At epoch 229 the loss is 1.527\n",
      "At epoch 230 the loss is 1.526\n",
      "At epoch 231 the loss is 1.525\n",
      "At epoch 232 the loss is 1.525\n",
      "At epoch 233 the loss is 1.524\n",
      "At epoch 234 the loss is 1.524\n",
      "At epoch 235 the loss is 1.524\n",
      "At epoch 236 the loss is 1.523\n",
      "At epoch 237 the loss is 1.523\n",
      "At epoch 238 the loss is 1.522\n",
      "At epoch 239 the loss is 1.522\n",
      "At epoch 240 the loss is 1.521\n",
      "At epoch 241 the loss is 1.521\n",
      "At epoch 242 the loss is 1.521\n",
      "At epoch 243 the loss is 1.52\n",
      "At epoch 244 the loss is 1.52\n",
      "At epoch 245 the loss is 1.519\n",
      "At epoch 246 the loss is 1.519\n",
      "At epoch 247 the loss is 1.519\n",
      "At epoch 248 the loss is 1.518\n",
      "At epoch 249 the loss is 1.518\n",
      "===========================================================\n",
      "Saving the model state\n",
      "Model state saved. It was completed in 0:00:00.005157\n",
      "===========================================================\n",
      "At epoch 250 the loss is 1.518\n",
      "At epoch 251 the loss is 1.517\n",
      "At epoch 252 the loss is 1.517\n",
      "At epoch 253 the loss is 1.517\n",
      "At epoch 254 the loss is 1.516\n",
      "At epoch 255 the loss is 1.516\n",
      "At epoch 256 the loss is 1.516\n",
      "At epoch 257 the loss is 1.515\n",
      "At epoch 258 the loss is 1.515\n",
      "At epoch 259 the loss is 1.515\n",
      "At epoch 260 the loss is 1.514\n",
      "At epoch 261 the loss is 1.514\n",
      "At epoch 262 the loss is 1.514\n",
      "At epoch 263 the loss is 1.514\n",
      "At epoch 264 the loss is 1.513\n",
      "At epoch 265 the loss is 1.513\n",
      "At epoch 266 the loss is 1.513\n",
      "At epoch 267 the loss is 1.512\n",
      "At epoch 268 the loss is 1.512\n",
      "At epoch 269 the loss is 1.512\n",
      "At epoch 270 the loss is 1.512\n",
      "At epoch 271 the loss is 1.511\n",
      "At epoch 272 the loss is 1.511\n",
      "At epoch 273 the loss is 1.511\n",
      "At epoch 274 the loss is 1.511\n",
      "At epoch 275 the loss is 1.51\n",
      "At epoch 276 the loss is 1.51\n",
      "At epoch 277 the loss is 1.51\n",
      "At epoch 278 the loss is 1.51\n",
      "At epoch 279 the loss is 1.51\n",
      "At epoch 280 the loss is 1.509\n",
      "At epoch 281 the loss is 1.509\n",
      "At epoch 282 the loss is 1.509\n",
      "At epoch 283 the loss is 1.509\n",
      "At epoch 284 the loss is 1.509\n",
      "At epoch 285 the loss is 1.508\n",
      "At epoch 286 the loss is 1.508\n",
      "At epoch 287 the loss is 1.508\n",
      "At epoch 288 the loss is 1.508\n",
      "At epoch 289 the loss is 1.508\n",
      "At epoch 290 the loss is 1.507\n",
      "At epoch 291 the loss is 1.507\n",
      "At epoch 292 the loss is 1.507\n",
      "At epoch 293 the loss is 1.507\n",
      "At epoch 294 the loss is 1.507\n",
      "At epoch 295 the loss is 1.506\n",
      "At epoch 296 the loss is 1.506\n",
      "At epoch 297 the loss is 1.506\n",
      "At epoch 298 the loss is 1.506\n",
      "At epoch 299 the loss is 1.506\n",
      "===========================================================\n",
      "Saving the model state\n",
      "Model state saved. It was completed in 0:00:00.009836\n",
      "===========================================================\n",
      "At epoch 300 the loss is 1.506\n",
      "Training finished.\n",
      "It took 0:00:40.983541 to finish training the model\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start2 = time.time()\n",
    "print(\"=================================================\")\n",
    "print(\"Collecting training data\")\n",
    "starte = time.time()\n",
    "data = get_training_data(text,window_size)\n",
    "ende = str(datetime.timedelta(seconds = time.time()-starte))\n",
    "print(\"It took {} to collect the data\".format(ende))\n",
    "print(\"The training data has {} target-context pairs\".format(len(data)))\n",
    "print(\"===================================================\")\n",
    "\n",
    "train(data,batch_size)\n",
    "\n",
    "end2 = str(datetime.timedelta(seconds = time.time()-start2))\n",
    "print(\"Training finished.\")\n",
    "print(\"It took {} to finish training the model\".format(end2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgQkaYstyj0Q"
   },
   "source": [
    "# 1.10 Train on the full dataset (0.5 points)\n",
    "\n",
    "Now, go back to 1.1 and remove the restriction on the number of sentences in your corpus. Then, reexecute code blocks 1.2, 1.3 and 1.6 (or those relevant if you created additional ones). \n",
    "\n",
    "* Then, retrain your model on the complete dataset.\n",
    "\n",
    "* Now, the input weights of the model contain the desired word embeddings! Save them together with the corresponding vocabulary items (Pytorch provides a nice [functionality](https://pytorch.org/tutorials/beginner/saving_loading_models.html) for this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "4x8hQP_bg4_g"
   },
   "outputs": [],
   "source": [
    "data_full = pd.read_csv('https://raw.githubusercontent.com/SouravDutta91/NNTI-WS2021-NLP-Project/main/data/hindi_hatespeech.tsv',sep='\\t')\n",
    "full_data = data_full['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Started Preprocessing\")\n",
    "cleantext_full = corpus_preprocess(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_full =  tokens(cleantext_full)\n",
    "text_full = text_full['text'].apply(lambda x: x.split())\n",
    "\n",
    "v = list(set(text_full.sum())) \n",
    "all_word = list(text_full.sum()) \n",
    "\n",
    "fullword_index = {word: i for i,word in enumerate(v)}\n",
    "fullindex_word = {i: word for i,word in enumerate(v)}\n",
    "\n",
    "data = get_training_data(text,window_size)\n",
    "train(data,batch_size)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NNTI_final_project_task_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}